{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samia\\anaconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.8)\n"
     ]
    }
   ],
   "source": [
    "#Beautiful Soup is a library that makes it easy to scrape information from web pages. \n",
    "#install beautifulsoup4 package\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google in c:\\users\\samia\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from google) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from beautifulsoup4->google) (1.8)\n"
     ]
    }
   ],
   "source": [
    "#The google package makes it easy to get results (URL links) of google search \n",
    "#install the google package\n",
    "!pip install google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\samia\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (5.2.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (5.1.1)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (6.1.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (3.4.4)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (2.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (4.7.1)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (4.3.4)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from newspaper3k) (2.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\samia\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2019.6.16)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\samia\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (41.0.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "#Newspaper is a Python3 library that makes automatic extraction articles\n",
    "#install newspaper3k package\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samia\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "from googlesearch import search\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import scraping_CBC as f1\n",
    "import scraping_OBJ as f2\n",
    "import scraping_Globalnews as f3\n",
    "import scraping_ottawacitizen as f4\n",
    "import scraping_torontosun as f5\n",
    "import scraping_nationalpost as f6\n",
    "import scraping_ottawamatters as f7\n",
    "\n",
    "record= []\n",
    "sincedate='05/01/2020'\n",
    "untildate='05/31/2020'\n",
    "query = 'ottawa AND ((oc AND transpo) OR lrt OR otrain)'\n",
    "\n",
    "for url in search (query,  tld='ca', tbs='cdr:1,cd_min:'+sincedate+',cd_max:'+untildate, lang='en',tpe=\"nws\", num=20, start=0, stop=100):\n",
    "    \n",
    "    try:\n",
    "            a = Article(url, language='en')\n",
    "            download = a.download()\n",
    "            parse = a.parse()\n",
    "            title = a.title\n",
    "            text = a.text\n",
    "            date = a.publish_date\n",
    "\n",
    "            if  text is None :  \n",
    "                    continue\n",
    "            \n",
    "            elif date is None :\n",
    "                \n",
    "                if  'https://www.cbc.ca/news' in url:\n",
    "                    publish_date=f1.publish_date(url)\n",
    "                    record.append((text,publish_date,title,url))\n",
    "\n",
    "\n",
    "                elif 'https://www.obj.ca/article/' in url:\n",
    "\n",
    "                    publish_date=f2.publish_date(url)\n",
    "                    record.append((text,publish_date,title,url))\n",
    "\n",
    "\n",
    "                elif 'https://globalnews.ca/' in url:\n",
    "                    publish_date=f3.publish_date(url)\n",
    "                    \n",
    "                    if 'Send this page' in text :\n",
    "                        text=f3.article(url)\n",
    "                        record.append((text,publish_date,title,url))\n",
    "    \n",
    "                elif 'https://ottawacitizen.com/' in url:\n",
    "                    publish_date=f4.publish_date(url) \n",
    "                    \n",
    "                    if 'Article content' in text :\n",
    "                        text=f4.article(url)\n",
    "                        record.append((text,publish_date,title,url))\n",
    "        \n",
    "                elif 'https://torontosun.com/' in url:\n",
    "\n",
    "                    publish_date=f5.publish_date(url)\n",
    "                    record.append((text,publish_date,title,url))\n",
    "\n",
    "\n",
    "                elif 'https://nationalpost.com/' in url:\n",
    "\n",
    "                    publish_date=f6.publish_date(url)\n",
    "                    record.append((text,publish_date,title,url))\n",
    "\n",
    "                elif 'https://ottawamatters.com/' in url:\n",
    "\n",
    "                    publish_date=f7.publish_date(url)\n",
    "                    record.append((text,publish_date,title,url))\n",
    "\n",
    "                else:    \n",
    "                    continue\n",
    "            \n",
    "\n",
    "            else : \n",
    "                publish_date = date.strftime(\"%m/%d/%Y\")\n",
    "                record.append((text, publish_date,title,url))\n",
    "\n",
    "    except:\n",
    "        pass \n",
    "    \n",
    "    df = pd.DataFrame(record, columns=['Article','Date', 'Heading','NewsType']) \n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincedate=\"2020-05-01\"\n",
    "untildate=\"2020-05-31\"\n",
    "filename='OCTranspo_newspapers_from_'+sincedate+'_until_'+untildate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(filename+\".xlsx\", encoding='utf-8',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
