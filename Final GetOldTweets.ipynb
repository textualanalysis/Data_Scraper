{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\samia\\anaconda3\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from GetOldTweets3) (4.3.4)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from GetOldTweets3) (1.4.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textblob in c:\\users\\samia\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in c:\\users\\samia\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\samia\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in c:\\users\\samia\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\samia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weeknumber function return the week number of month by passing date as a parameter\n",
    "def weeknum(d):\n",
    "\n",
    "    if d >= 22:\n",
    "        weeknum = 4\n",
    "    elif (d >= 15):\n",
    "        weeknum = 3\n",
    "    elif (d > 7):\n",
    "        weeknum = 2\n",
    "    else: \n",
    "        weeknum = 1\n",
    "    return (str(weeknum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincedate=\"2020-07-01\"\n",
    "untildate=\"2020-08-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location ='C:/Users/KSA/new script/dataset/OCtranspo/Tweets/'\n",
    "#file_location ='C:/Users/KSA/new script/dataset/Quayside_Toronto/Tweets/'\n",
    "\n",
    "#file_location ='C:/Users/samia/new script/dataset/Covid19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=file_location+'OCtranspo_from_'+sincedate+'_until_'+untildate\n",
    "#filename=file_location+'Quayside_Toronto_from_'+sincedate+'_until_'+untildate\n",
    "#filename=file_location+'Tweets_Covid19_Toronto_from_'+sincedate+'_until_'+untildate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words='ottawa AND ((oc AND transpo) OR lrt OR otrain)'\n",
    "#key_words='toronto AND (quayside OR blocksidewalk)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weeknum(d):\n",
    "        if d >= 22:\n",
    "            weeknum = 4\n",
    "        elif (d >= 15):\n",
    "            weeknum = 3\n",
    "        elif (d > 7):\n",
    "            weeknum = 2\n",
    "        else: \n",
    "            weeknum = 1\n",
    "        return (str(weeknum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured during an HTTP request: HTTP Error 404: Not Found\n",
      "Try to open in browser: https://twitter.com/search?q=ottawa%20AND%20%28%28oc%20AND%20transpo%29%20OR%20lrt%20OR%20otrain%29%20since%3A2019-07-01%20until%3A2020-10-01&src=typd\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samia\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    import GetOldTweets3 as got\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    sincedate=\"2019-07-01\"\n",
    "    untildate=\"2020-10-01\"\n",
    "    \n",
    "\n",
    "    tweetCriteria = got.manager.TweetCriteria().setLang('en').setQuerySearch('ottawa AND ((oc AND transpo) OR lrt OR otrain)') \\\n",
    "        .setSince(sincedate) \\\n",
    "        .setUntil(untildate) \\\n",
    "        .setEmoji(\"unicode\") \\\n",
    "        .setMaxTweets(10000000)\n",
    "    # Creation of list that contains all tweets\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    # Creating list of chosen tweet data\n",
    "    tweet_detail = [[tweet.username,tweet.text,tweet.date.strftime(\"%m/%d/%Y %H:%M:%S\"),\"Month_\"+ tweet.date.strftime(\"%m\")+\" Week \"+weeknum(int(tweet.date.strftime(\"%d\"))),tweet.date.strftime(\"%m\")] for tweet in tweets]\n",
    "    dataset = pd.DataFrame(data=tweet_detail, columns=['username','text', 'date','Week','Month'])\n",
    "    file_location ='C:/Users/KSA/new script/dataset/OCtranspo/Tweets/'\n",
    "    #file_location ='C:/Users/KSA/new script/dataset/Quayside_Toronto/Tweets/'\n",
    "    #file_location ='C:/Users/samia/new script/dataset/Covid19'\n",
    "    filename=file_location+'OCtranspo_from_'+sincedate+'_until_'+untildate\n",
    "    #filename=file_location+'Quayside_Toronto_from_'+sincedate+'_until_'+untildate\n",
    "    #filename=file_location+'Tweets_Covid19_Toronto_from_'+sincedate+'_until_'+untildate\n",
    "    dataset.to_excel(filename+'.xlsx' ,encoding='utf-8',index=False) \n",
    "    dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_textual(fname):\n",
    "    #Importing libraries\n",
    "\n",
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "    from textblob import Word\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import TweetTokenizer \n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from unidecode import unidecode\n",
    "           \n",
    "    # importing pandas package \n",
    "    import pandas as pd    \n",
    "    df = pd.read_excel(fname+'.xlsx') \n",
    "    # Converting to lowercase\n",
    " \n",
    "    df['tweet_clean_textual']= df['text'].str.lower()\n",
    "    \n",
    "    \n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].fillna(\"\") \n",
    "\n",
    "    def deEmojify(inputString):\n",
    "        return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        \n",
    "    # Removing emoji\n",
    "    df['tweet_clean_textual'] = df['tweet_clean_textual'].apply(lambda x: deEmojify(x))\n",
    "\n",
    "    #Removing HTTP links\n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.replace(r\"(https?\\://)\\S+\", ' ')   \n",
    "                             \n",
    "    #Removing WWW links    \n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.replace(r'www.[A-Za-z0-9./?//:]\\S+',' ') \n",
    "\n",
    "    # Removing hashtags\n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.replace(r'#[A-Za-z0-9]\\S+', ' ')\n",
    "\n",
    "    # Removing mentions  \n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.replace(r'@[A-Za-z0-9]\\S+', ' ')\n",
    "\n",
    "    #removing numbers\n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.replace(r'[0-9]+', ' ')\n",
    "\n",
    "    #removing special character \n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.replace(r'[^a-zA-Z0-9\\s]+', ' ')\n",
    "\n",
    "    #removing extra whitespaces\n",
    "    df['tweet_clean_textual']= df['tweet_clean_textual'].str.strip()\n",
    "\n",
    "    # Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get Part-of-speech (pos) tag of words of the sentence \n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    # Lemmatize a Sentence with the appropriate Part-of-speech tag\n",
    "    df['tweet_clean_textual'] = df['tweet_clean_textual'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x.split()]))\n",
    "\n",
    "    #remove_stop_words using NLTK library\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df['tweet_clean_textual']=df['tweet_clean_textual'].apply(lambda x: \" \".join([item for item in x.split() if item not in stop]))\n",
    "\n",
    "\n",
    "    #Remove short words with less than 3 letters and remove_stop_words using  txt file\n",
    "    def remove_short_stop_words(row):\n",
    "        review = row['tweet_clean_textual']\n",
    "        #Tokenize the sentence by using word_tokenize method from nltk library\n",
    "        tokens = TweetTokenizer().tokenize(review)\n",
    "        token_words = [w for w in tokens if len(w) >= 3]\n",
    "        with open(\"additional_stop_words.txt\",'r') as stopwordFile:\n",
    "            #Tokenize the sentence by using split() method  from String library\n",
    "            b=stopwordFile.read().split() \n",
    "            #remove_stop_words using txt file (additinal_stop_words)\n",
    "            stop_words = [word for word in token_words if word not in b]\n",
    "            joined_words = ( \" \".join(stop_words))\n",
    "        \n",
    "            stopwordFile.close()\n",
    "\n",
    "            return joined_words\n",
    "\n",
    "    # Call the remove_short_words  function\n",
    "    df['tweet_clean_textual'] = df.apply(remove_short_stop_words, axis=1)\n",
    "    df.to_excel(filename+'.xlsx',encoding='utf-8',index=False) \n",
    "\n",
    "    # display \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/samia/new script/dataset/Covid19Covid19_Toronto_from_2019-07-01_until_2020-08-01.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c6deab7203b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleaning_textual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-0bab2027bb18>\u001b[0m in \u001b[0;36mcleaning_textual\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# importing pandas package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Converting to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     return io.parse(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/samia/new script/dataset/Covid19Covid19_Toronto_from_2019-07-01_until_2020-08-01.xlsx'"
     ]
    }
   ],
   "source": [
    "cleaning_textual(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: emoji in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentiment(fname):\n",
    "    #Importing libraries\n",
    "\n",
    "    import re\n",
    "    import string\n",
    "    import emoji\n",
    "    import nltk\n",
    "    from textblob import Word\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    \n",
    "    \n",
    "    # Happy Emoticons\n",
    "    emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "    # Sad Emoticons\n",
    "    emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', ':3', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "    \n",
    "    # importing pandas package \n",
    "    import pandas as pd \n",
    "    \n",
    "    df = pd.read_excel(fname+'.xlsx') \n",
    "    # Converting to lowercase\n",
    " \n",
    "    df['tweet_clean_sentiment']= df['text'].str.lower()\n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].fillna(\"\") \n",
    "    \n",
    "    # Function for converting emoticons to sentiments (happy or sad)\n",
    "    def convert_emoticons(row):\n",
    "        text = row['tweet_clean_sentiment']\n",
    "        tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "        for word in tweet_tokenizer.tokenize(text):\n",
    "           \n",
    "            if word in emoticons_sad:         \n",
    "                text = text.replace( word, ' sad ') \n",
    "                \n",
    "            elif word in emoticons_happy:\n",
    "                text = text.replace( word, ' happy ')\n",
    "        return text\n",
    "    \n",
    "    # Convert emoticons to words {sad or happy}\n",
    "    df['tweet_clean_sentiment'] = df.apply(convert_emoticons, axis=1)\n",
    "    \n",
    "    # Convert emoji to words\n",
    "    df['tweet_clean_sentiment'] = df['tweet_clean_sentiment'].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "    #replace apostrophe (’) by (') then apply the decontracted function \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'\\’', '\\'')\n",
    "\n",
    "    # Expending  contraction function\n",
    "    def decontracted(row):\n",
    "        sentence=row['tweet_clean_sentiment'] \n",
    "        # specific\n",
    "        sentence = re.sub(r\"won\\'t\", \"will not\", sentence)       \n",
    "        sentence = re.sub(r\"can\\'t\", \"cannot\", sentence)\n",
    "        sentence = re.sub(r\"shan\\'t\", \"shall not\", sentence)\n",
    "        sentence = re.sub(r\"wanna\", \"want to\", sentence) \n",
    "        sentence = re.sub(r\"let's\", \"let us\", sentence)\n",
    "        sentence = re.sub(r\"innit\", \"is it not\", sentence)\n",
    "        sentence = re.sub(r\"gonna\", \"going to\", sentence)\n",
    "        sentence = re.sub(r\"gotta\", \"got to\", sentence)\n",
    "\n",
    "        # general\n",
    "        sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "        sentence = re.sub(r\"\\'d\", \" would\", sentence)   \n",
    "        #sentence = re.sub(r\"\\'d\", \" had\", sentence)    \n",
    "        sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "        #sentence = re.sub(r\"\\'ll\", \" shall\", sentence)\n",
    "        sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "        sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "        sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "        \n",
    "        return sentence\n",
    "    # Expending  contraction\n",
    "    df['tweet_clean_sentiment'] = df.apply(decontracted, axis=1)\n",
    "    \n",
    "    #Removing HTTP links\n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r\"(https?\\://)\\S+\", ' ')   \n",
    "                             \n",
    "    #Removing WWW links    \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'www.[A-Za-z0-9./?//:]\\S+',' ') \n",
    "\n",
    "    # Removing hashtags\n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'#[A-Za-z0-9]\\S+', ' ')\n",
    "\n",
    "    # Removing mentions  \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'@[A-Za-z0-9]\\S+', ' ')\n",
    "\n",
    "    #removing numbers\n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'[0-9]+', ' ')\n",
    "\n",
    "    #removing special character \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'[^a-zA-Z0-9-\\s]+', ' ')\n",
    "\n",
    "    #removing extra whitespaces\n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.strip()\n",
    "\n",
    "    # Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get Part-of-speech (pos) tag of words of the sentence \n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    # Lemmatize a Sentence with the appropriate Part-of-speech tag\n",
    "    df['tweet_clean_sentiment'] = df['tweet_clean_sentiment'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x.split()]))\n",
    "\n",
    "    # Connect with NO \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'no ', 'no_')\n",
    "\n",
    "    # Connect with NOT \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'not ', 'not_')\n",
    "\n",
    "\n",
    "    #Remove stop_words using stop txt file\n",
    "    def remove_stop_words(row):\n",
    "        with open(\"stop.txt\",'r') as stopwordFile:\n",
    "            #Tokenize the sentence by using split() method  from String library\n",
    "            b=stopwordFile.read().split() \n",
    "            review = row['tweet_clean_sentiment']\n",
    "            tokens = TweetTokenizer().tokenize(review)\n",
    "            stop_words = [word for word in tokens if word not in b]\n",
    "            remove_words = ( \" \".join(stop_words))\n",
    "            stopwordFile.close()\n",
    "            return remove_words\n",
    "        \n",
    "        # Call the remove_stop_words function   \n",
    "    df['tweet_clean_sentiment'] = df.apply(remove_stop_words, axis=1)\n",
    "\n",
    "    #Remove short words with less than 3 letters\n",
    "    def remove_short_words(row):\n",
    "        review = row['tweet_clean_sentiment']\n",
    "        #Tokenize the sentence by using word_tokenize method from nltk library\n",
    "        tokens = TweetTokenizer().tokenize(review)\n",
    "        token_words = [w for w in tokens if len(w) >= 3]\n",
    "        joined_words = ( \" \".join(token_words))\n",
    "        return joined_words\n",
    "\n",
    "    # Call the remove_short_words function\n",
    "    df['tweet_clean_sentiment'] = df.apply(remove_short_words, axis=1)\n",
    "  \n",
    "    # Replace hyphen \"-\" with underscore \"_\". \n",
    "    df['tweet_clean_sentiment']= df['tweet_clean_sentiment'].str.replace(r'-', '_')\n",
    "    df.to_excel(filename+'TextualSentimentclean.xlsx',encoding='utf-8',index=False) \n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Week</th>\n",
       "      <th>Month</th>\n",
       "      <th>tweet_clean_textual</th>\n",
       "      <th>tweet_clean_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joe_cressy</td>\n",
       "      <td>While Toronto began as a Waterfront City, over...</td>\n",
       "      <td>12/31/2019 14:52:18</td>\n",
       "      <td>Month_12 Week 4</td>\n",
       "      <td>12</td>\n",
       "      <td>toronto begin waterfront city time lose expres...</td>\n",
       "      <td>toronto begin waterfront city over time lose e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laurx16</td>\n",
       "      <td>if anyone has any more examples of this in tor...</td>\n",
       "      <td>12/30/2019 02:19:02</td>\n",
       "      <td>Month_12 Week 4</td>\n",
       "      <td>12</td>\n",
       "      <td>example toronto specifically quayside portland...</td>\n",
       "      <td>anyone any more example toronto specifically q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheGeospatial</td>\n",
       "      <td>Sidewalk Labs will collect and manage data on ...</td>\n",
       "      <td>12/26/2019 12:29:53</td>\n",
       "      <td>Month_12 Week 4</td>\n",
       "      <td>12</td>\n",
       "      <td>sidewalk lab collect manage data rubbish dispo...</td>\n",
       "      <td>sidewalk lab will collect manage data rubbish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ChrisWTaxTech</td>\n",
       "      <td>Sidewalk Labs listed details about how it plan...</td>\n",
       "      <td>12/23/2019 09:46:31</td>\n",
       "      <td>Month_12 Week 4</td>\n",
       "      <td>12</td>\n",
       "      <td>sidewalk lab list detail plan collect use prop...</td>\n",
       "      <td>sidewalk lab list detail plan collect use prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keerthanarang</td>\n",
       "      <td>.@AndrewMillerYYZ works on mobility and has be...</td>\n",
       "      <td>12/20/2019 17:25:01</td>\n",
       "      <td>Month_12 Week 3</td>\n",
       "      <td>12</td>\n",
       "      <td>work mobility look way design street quayside ...</td>\n",
       "      <td>work mobility look way can design street quays...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>keerthanarang</td>\n",
       "      <td>As my colleague @micahlasher says, it’s rarely...</td>\n",
       "      <td>12/20/2019 16:59:59</td>\n",
       "      <td>Month_12 Week 3</td>\n",
       "      <td>12</td>\n",
       "      <td>colleague say rarely straight line big complex...</td>\n",
       "      <td>colleague say rarely straight line big complex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neve_peric</td>\n",
       "      <td>Waterfront Toronto's partner ... #blocksidewalk</td>\n",
       "      <td>12/17/2019 22:27:18</td>\n",
       "      <td>Month_12 Week 3</td>\n",
       "      <td>12</td>\n",
       "      <td>waterfront toronto partner</td>\n",
       "      <td>waterfront toronto partner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gvwilson</td>\n",
       "      <td>This is the company Toronto wants to hand an e...</td>\n",
       "      <td>12/17/2019 15:57:25</td>\n",
       "      <td>Month_12 Week 3</td>\n",
       "      <td>12</td>\n",
       "      <td>company toronto hand entire neighborhood suppo...</td>\n",
       "      <td>company toronto want hand entire neighborhood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gvwilson</td>\n",
       "      <td>The engineers and managers who have passively ...</td>\n",
       "      <td>12/17/2019 15:20:24</td>\n",
       "      <td>Month_12 Week 3</td>\n",
       "      <td>12</td>\n",
       "      <td>engineer manager passively accede one suppose ...</td>\n",
       "      <td>engineer manager who passively accede one supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Heretic270</td>\n",
       "      <td>This is what you guys need to know. The ultima...</td>\n",
       "      <td>12/15/2019 05:40:59</td>\n",
       "      <td>Month_12 Week 3</td>\n",
       "      <td>12</td>\n",
       "      <td>guy need know ultimate bos sidewalk larry page...</td>\n",
       "      <td>what guy need know ultimate bos sidewalk larry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blocksidewalk</td>\n",
       "      <td>“This makes me think #Toronto was right to go ...</td>\n",
       "      <td>12/14/2019 12:32:19</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>make think right slow shouldnt rush sidewalk e...</td>\n",
       "      <td>make think right slow should not_rush sidewalk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JPWP</td>\n",
       "      <td>If there’s a ‘culture of fear’ &amp; ‘waning trans...</td>\n",
       "      <td>12/13/2019 18:03:47</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>culture fear wan transparency inside vendor fi...</td>\n",
       "      <td>culture fear wan transparency inside vendor fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blocksidewalk</td>\n",
       "      <td>“In Toronto, Google’s Sidewalk Labs is slowly ...</td>\n",
       "      <td>12/13/2019 13:02:25</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>toronto google sidewalk lab slowly privatize m...</td>\n",
       "      <td>toronto google sidewalk lab slowly privatize m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KnowToronto</td>\n",
       "      <td>The Quayside project in Toronto has courted mo...</td>\n",
       "      <td>12/12/2019 18:56:29</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>quayside project toronto court controversy exc...</td>\n",
       "      <td>quayside project toronto court more controvers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ntoomesense</td>\n",
       "      <td>NO PRIVACY 2019 Alphabet Inc. is determined to...</td>\n",
       "      <td>12/12/2019 18:52:08</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>privacy alphabet inc determine smart city toro...</td>\n",
       "      <td>no_privacy alphabet inc determine smart city t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OpenCityNet</td>\n",
       "      <td>2. This is about tech, cities, and democracy. ...</td>\n",
       "      <td>12/11/2019 12:14:55</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>tech city democracy quayside project toronto r...</td>\n",
       "      <td>tech city democracy quayside project toronto r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ABI_Bonte</td>\n",
       "      <td>Alphabet's Sidewalk Labs and Plaza Ventures es...</td>\n",
       "      <td>12/10/2019 13:35:16</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>alphabet sidewalk lab plaza venture establish ...</td>\n",
       "      <td>alphabet sidewalk lab plaza venture establish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>biancawylie</td>\n",
       "      <td>The Digital Infrastructure Plan will ultimatel...</td>\n",
       "      <td>12/10/2019 00:14:44</td>\n",
       "      <td>Month_12 Week 2</td>\n",
       "      <td>12</td>\n",
       "      <td>digital infrastructure plan ultimately use ass...</td>\n",
       "      <td>digital infrastructure plan will ultimately us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EmilSkandul</td>\n",
       "      <td>.@sidewalklabs and @DanDoctoroff's Toronto Qua...</td>\n",
       "      <td>12/06/2019 14:31:19</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>toronto quayside model city future develop spe...</td>\n",
       "      <td>toronto quayside model city future develop spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OliverRTennant</td>\n",
       "      <td>Great talk this morning with @jshapes &amp;amp; @b...</td>\n",
       "      <td>12/05/2019 17:36:18</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>great talk morning amp future development quay...</td>\n",
       "      <td>great talk morning with amp future development...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>books_pop</td>\n",
       "      <td>Finally reading Age of Surveillance Capitalism...</td>\n",
       "      <td>12/04/2019 21:19:31</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>finally reading age surveillance capitalism ea...</td>\n",
       "      <td>finally reading age surveillance capitalism ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TenantNet</td>\n",
       "      <td>Dan Doctoroff gets exposed again. He flubbed N...</td>\n",
       "      <td>12/04/2019 15:10:02</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>dan doctoroff expose flubbed nyc shrivel bosto...</td>\n",
       "      <td>dan doctoroff get expose again flubbed nyc shr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>juliedilorenzo3</td>\n",
       "      <td>@WaterfrontTO Hard costs affordable housing To...</td>\n",
       "      <td>12/04/2019 11:07:22</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>hard cost affordable housing toronto best spen...</td>\n",
       "      <td>hard cost affordable housing toronto best spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>visheem</td>\n",
       "      <td>Sidewalk Labs dangles $10M venture capital fun...</td>\n",
       "      <td>12/02/2019 15:57:43</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>sidewalk lab dangles venture capital fund toro...</td>\n",
       "      <td>sidewalk lab dangles venture capital fund toro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JPWP</td>\n",
       "      <td>Valuable additional context on Alphabet/Google...</td>\n",
       "      <td>12/02/2019 02:32:02</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>valuable additional context alphabet google sh...</td>\n",
       "      <td>valuable additional context alphabet google sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>realOOLF</td>\n",
       "      <td>Very special new ep today! one of our fav gues...</td>\n",
       "      <td>12/01/2019 18:49:06</td>\n",
       "      <td>Month_12 Week 1</td>\n",
       "      <td>12</td>\n",
       "      <td>special new today one fav guest back show disc...</td>\n",
       "      <td>very special new today one fav guest back show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Verisk</td>\n",
       "      <td>#HudsonYards in New York and @SidewalkLabs’ Qu...</td>\n",
       "      <td>11/30/2019 08:52:00</td>\n",
       "      <td>Month_11 Week 4</td>\n",
       "      <td>11</td>\n",
       "      <td>new york quayside project toronto show cusp ma...</td>\n",
       "      <td>new york quayside project toronto show cusp ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Jasonvalliere2</td>\n",
       "      <td>Sidewalk Labs dangles $10M for venture capital...</td>\n",
       "      <td>11/29/2019 01:00:06</td>\n",
       "      <td>Month_11 Week 4</td>\n",
       "      <td>11</td>\n",
       "      <td>sidewalk lab dangles venture capital fund toro...</td>\n",
       "      <td>sidewalk lab dangles venture capital fund toro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>OPIRGToronto</td>\n",
       "      <td>We're joining the @blocksidewalk labs campaign...</td>\n",
       "      <td>11/28/2019 17:02:49</td>\n",
       "      <td>Month_11 Week 4</td>\n",
       "      <td>11</td>\n",
       "      <td>join lab campaign toronto part include support...</td>\n",
       "      <td>join lab campaign toronto part include support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>anne_nonymity</td>\n",
       "      <td>Sidewalk Labs dangles $10M for venture capital...</td>\n",
       "      <td>11/28/2019 16:26:57</td>\n",
       "      <td>Month_11 Week 4</td>\n",
       "      <td>11</td>\n",
       "      <td>sidewalk lab dangles venture capital fund toro...</td>\n",
       "      <td>sidewalk lab dangles venture capital fund toro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>therealestatetv</td>\n",
       "      <td>The company produced a plan that went far beyo...</td>\n",
       "      <td>07/15/2019 00:06:23</td>\n",
       "      <td>Month_07 Week 3</td>\n",
       "      <td>7</td>\n",
       "      <td>company produce plan far beyond acre quayside ...</td>\n",
       "      <td>company produce plan far beyond acre quayside ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>HenrikBechmann</td>\n",
       "      <td>Caution ⚡️⚡️⚡️p.96: \"...while recognizing that...</td>\n",
       "      <td>07/14/2019 18:57:38</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>caution recognize waterfront toronto receive v...</td>\n",
       "      <td>caution high voltage high voltage high voltage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>smartcityvwpts</td>\n",
       "      <td>On June 17, 2019 #SidewalkLabs submitted their...</td>\n",
       "      <td>07/14/2019 18:32:00</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>submit proposal call master innovation develop...</td>\n",
       "      <td>submit proposal call master innovation develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>JPWP</td>\n",
       "      <td>“Toronto Tomorrow” | MetaFilter http://www.met...</td>\n",
       "      <td>07/14/2019 14:03:09</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>toronto tomorrow metafilter</td>\n",
       "      <td>toronto tomorrow metafilter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>ntoomesense</td>\n",
       "      <td>But I think … every Canadian is at risk right ...</td>\n",
       "      <td>07/13/2019 18:54:01</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>think canadian risk right third party able acc...</td>\n",
       "      <td>think canadian risk right third party would ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>joe_cressy</td>\n",
       "      <td>Waterfront Toronto is hosting a series of publ...</td>\n",
       "      <td>07/12/2019 15:13:32</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>waterfront toronto host series public consulta...</td>\n",
       "      <td>waterfront toronto host series public consulta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>mcfslaw</td>\n",
       "      <td>“There are huge and long lasting implications ...</td>\n",
       "      <td>07/12/2019 13:51:35</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>huge long last implication smartcities convers...</td>\n",
       "      <td>huge long last implication smartcities convers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>JPWP</td>\n",
       "      <td>Sounds like a fairly accurate description of t...</td>\n",
       "      <td>07/12/2019 13:20:20</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>sound like fairly accurate description organic...</td>\n",
       "      <td>sound like fairly accurate description organic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>blocksidewalk</td>\n",
       "      <td>“90% more land to prove the concept? WTF !” Co...</td>\n",
       "      <td>07/12/2019 00:06:58</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>land prove concept wtf comment left resident v...</td>\n",
       "      <td>more land prove concept wtf comment left resid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>blocksidewalk</td>\n",
       "      <td>Thank you, Berlin friends! You showed us it ca...</td>\n",
       "      <td>07/11/2019 22:03:02</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>thank berlin friend show queen show barcelona ...</td>\n",
       "      <td>thank berlin friend show can queen show can ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>DianaYoonTO</td>\n",
       "      <td>This will be true for Google in Toronto as wel...</td>\n",
       "      <td>07/11/2019 16:55:02</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>true google toronto well need give away public...</td>\n",
       "      <td>will true google toronto well not_need give aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>SidewalkToronto</td>\n",
       "      <td>This summer, @WaterfrontTO will be hosting a s...</td>\n",
       "      <td>07/11/2019 15:05:18</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>summer host series public consultation across ...</td>\n",
       "      <td>summer will host series public consultation ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>bowker_john</td>\n",
       "      <td>Who benefits from this botnet promoting the Si...</td>\n",
       "      <td>07/11/2019 13:37:46</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>benefit botnet promote sidewalk lab toronto pr...</td>\n",
       "      <td>who benefit botnet promote sidewalk lab toront...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>YQNA</td>\n",
       "      <td>Waterfront Toronto to host public consultation...</td>\n",
       "      <td>07/11/2019 12:01:12</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>waterfront toronto host public consultation si...</td>\n",
       "      <td>waterfront toronto host public consultation si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>ODS_Directory</td>\n",
       "      <td>Find out the AILA 2019 Tasmanian Award Winners...</td>\n",
       "      <td>07/11/2019 06:10:01</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>find aila tasmanian award winner read excite q...</td>\n",
       "      <td>find out aila tasmanian award winner read exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>_w0bb1t_</td>\n",
       "      <td>Solidarity to #blocksidewalk from #fuckoffgoog...</td>\n",
       "      <td>07/11/2019 05:39:33</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>solidarity amp struggle people lab attempt bui...</td>\n",
       "      <td>solidarity amp struggle people against lab att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>newyorkgreen</td>\n",
       "      <td>Honored to host a presentation here @BEExNY to...</td>\n",
       "      <td>07/10/2019 22:22:05</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>honor host presentation tonight charlotte matt...</td>\n",
       "      <td>honor host presentation tonight charlotte matt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>CultureofCities</td>\n",
       "      <td>So San Francisco resents the techies and the t...</td>\n",
       "      <td>07/10/2019 21:37:25</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>san francisco resents techie techie hate fate ...</td>\n",
       "      <td>san francisco resents techie techie hate fate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>DDavoult</td>\n",
       "      <td>Toronto: the Sidewalk Labs master plan for Qua...</td>\n",
       "      <td>07/10/2019 15:53:17</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>toronto sidewalk lab master plan quayside prov...</td>\n",
       "      <td>toronto sidewalk lab master plan quayside prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>AmieDevero</td>\n",
       "      <td>#Alphabet Releases Its Master Plan for Toronto...</td>\n",
       "      <td>07/10/2019 14:05:17</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>release master plan toronto new</td>\n",
       "      <td>release master plan toronto new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>DVN17Team</td>\n",
       "      <td>Waterfront Toronto wants your input on @sidewa...</td>\n",
       "      <td>07/10/2019 13:35:56</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>waterfront toronto input proposal quayside kno...</td>\n",
       "      <td>waterfront toronto want input proposal quaysid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>juliedilorenzo3</td>\n",
       "      <td>@WaterfrontTO @keerthanarang @MicahLasher @blo...</td>\n",
       "      <td>07/10/2019 11:07:39</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>pretty good reading pda say collaboration city...</td>\n",
       "      <td>pretty good reading pda where say collaboratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>petco68</td>\n",
       "      <td>Toronto’s city council warning them that Quays...</td>\n",
       "      <td>07/10/2019 09:10:36</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>toronto city council warn quayside highly evol...</td>\n",
       "      <td>toronto city council warn quayside most highly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>RocheJim</td>\n",
       "      <td>But Gord, they heard from \"folks\". #BlockSidew...</td>\n",
       "      <td>07/10/2019 02:31:28</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>gord heard folk</td>\n",
       "      <td>gord heard folk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>joe_cressy</td>\n",
       "      <td>Tonight Waterfront Toronto is hosting our Quay...</td>\n",
       "      <td>07/09/2019 23:14:11</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>tonight waterfront toronto host quayside stake...</td>\n",
       "      <td>tonight waterfront toronto host quayside stake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>TOproh</td>\n",
       "      <td>Lol venture capital firm says Sidewalk’s Quays...</td>\n",
       "      <td>07/09/2019 16:17:38</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>lol venture capital firm say sidewalk quayside...</td>\n",
       "      <td>lol venture capital firm say sidewalk quayside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>acserrano</td>\n",
       "      <td>Pleased to hear that our innovation ecosystem ...</td>\n",
       "      <td>07/09/2019 16:07:31</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>pleased hear innovation ecosystem toronto stil...</td>\n",
       "      <td>pleased hear innovation ecosystem toronto stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>HeidiBoghosian</td>\n",
       "      <td>@AbiHassen discusses upsides of #ImpeachTrumpN...</td>\n",
       "      <td>07/09/2019 15:38:01</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>discus upside first half show cover propose sm...</td>\n",
       "      <td>discus upside first half show cover propose sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>barbckellydc</td>\n",
       "      <td>FPF Member News: Sidewalk Labs Releases Detail...</td>\n",
       "      <td>07/09/2019 04:42:55</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>fpf member news sidewalk lab release detailed ...</td>\n",
       "      <td>fpf member news sidewalk lab release detailed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>JPWP</td>\n",
       "      <td>Hey #Toronto: “there’s a doin’s a transpirin’”...</td>\n",
       "      <td>07/09/2019 00:43:20</td>\n",
       "      <td>Month_07 Week 2</td>\n",
       "      <td>7</td>\n",
       "      <td>hey doins transpirin reality alphabet google s...</td>\n",
       "      <td>hey doin transpirin reality alphabet google sw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            username                                               text  \\\n",
       "0         joe_cressy  While Toronto began as a Waterfront City, over...   \n",
       "1            laurx16  if anyone has any more examples of this in tor...   \n",
       "2      TheGeospatial  Sidewalk Labs will collect and manage data on ...   \n",
       "3      ChrisWTaxTech  Sidewalk Labs listed details about how it plan...   \n",
       "4      keerthanarang  .@AndrewMillerYYZ works on mobility and has be...   \n",
       "5      keerthanarang  As my colleague @micahlasher says, it’s rarely...   \n",
       "6         neve_peric   Waterfront Toronto's partner ... #blocksidewalk    \n",
       "7           gvwilson  This is the company Toronto wants to hand an e...   \n",
       "8           gvwilson  The engineers and managers who have passively ...   \n",
       "9         Heretic270  This is what you guys need to know. The ultima...   \n",
       "10     blocksidewalk  “This makes me think #Toronto was right to go ...   \n",
       "11              JPWP  If there’s a ‘culture of fear’ & ‘waning trans...   \n",
       "12     blocksidewalk  “In Toronto, Google’s Sidewalk Labs is slowly ...   \n",
       "13       KnowToronto  The Quayside project in Toronto has courted mo...   \n",
       "14       ntoomesense  NO PRIVACY 2019 Alphabet Inc. is determined to...   \n",
       "15       OpenCityNet  2. This is about tech, cities, and democracy. ...   \n",
       "16         ABI_Bonte  Alphabet's Sidewalk Labs and Plaza Ventures es...   \n",
       "17       biancawylie  The Digital Infrastructure Plan will ultimatel...   \n",
       "18       EmilSkandul  .@sidewalklabs and @DanDoctoroff's Toronto Qua...   \n",
       "19    OliverRTennant  Great talk this morning with @jshapes &amp; @b...   \n",
       "20         books_pop  Finally reading Age of Surveillance Capitalism...   \n",
       "21         TenantNet  Dan Doctoroff gets exposed again. He flubbed N...   \n",
       "22   juliedilorenzo3  @WaterfrontTO Hard costs affordable housing To...   \n",
       "23           visheem  Sidewalk Labs dangles $10M venture capital fun...   \n",
       "24              JPWP  Valuable additional context on Alphabet/Google...   \n",
       "25          realOOLF  Very special new ep today! one of our fav gues...   \n",
       "26            Verisk  #HudsonYards in New York and @SidewalkLabs’ Qu...   \n",
       "27    Jasonvalliere2  Sidewalk Labs dangles $10M for venture capital...   \n",
       "28      OPIRGToronto  We're joining the @blocksidewalk labs campaign...   \n",
       "29     anne_nonymity  Sidewalk Labs dangles $10M for venture capital...   \n",
       "..               ...                                                ...   \n",
       "571  therealestatetv  The company produced a plan that went far beyo...   \n",
       "572   HenrikBechmann  Caution ⚡️⚡️⚡️p.96: \"...while recognizing that...   \n",
       "573   smartcityvwpts  On June 17, 2019 #SidewalkLabs submitted their...   \n",
       "574             JPWP  “Toronto Tomorrow” | MetaFilter http://www.met...   \n",
       "575      ntoomesense  But I think … every Canadian is at risk right ...   \n",
       "576       joe_cressy  Waterfront Toronto is hosting a series of publ...   \n",
       "577          mcfslaw  “There are huge and long lasting implications ...   \n",
       "578             JPWP  Sounds like a fairly accurate description of t...   \n",
       "579    blocksidewalk  “90% more land to prove the concept? WTF !” Co...   \n",
       "580    blocksidewalk  Thank you, Berlin friends! You showed us it ca...   \n",
       "581      DianaYoonTO  This will be true for Google in Toronto as wel...   \n",
       "582  SidewalkToronto  This summer, @WaterfrontTO will be hosting a s...   \n",
       "583      bowker_john  Who benefits from this botnet promoting the Si...   \n",
       "584             YQNA  Waterfront Toronto to host public consultation...   \n",
       "585    ODS_Directory  Find out the AILA 2019 Tasmanian Award Winners...   \n",
       "586         _w0bb1t_  Solidarity to #blocksidewalk from #fuckoffgoog...   \n",
       "587     newyorkgreen  Honored to host a presentation here @BEExNY to...   \n",
       "588  CultureofCities  So San Francisco resents the techies and the t...   \n",
       "589         DDavoult  Toronto: the Sidewalk Labs master plan for Qua...   \n",
       "590       AmieDevero  #Alphabet Releases Its Master Plan for Toronto...   \n",
       "591        DVN17Team  Waterfront Toronto wants your input on @sidewa...   \n",
       "592  juliedilorenzo3  @WaterfrontTO @keerthanarang @MicahLasher @blo...   \n",
       "593          petco68  Toronto’s city council warning them that Quays...   \n",
       "594         RocheJim  But Gord, they heard from \"folks\". #BlockSidew...   \n",
       "595       joe_cressy  Tonight Waterfront Toronto is hosting our Quay...   \n",
       "596           TOproh  Lol venture capital firm says Sidewalk’s Quays...   \n",
       "597        acserrano  Pleased to hear that our innovation ecosystem ...   \n",
       "598   HeidiBoghosian  @AbiHassen discusses upsides of #ImpeachTrumpN...   \n",
       "599     barbckellydc  FPF Member News: Sidewalk Labs Releases Detail...   \n",
       "600             JPWP  Hey #Toronto: “there’s a doin’s a transpirin’”...   \n",
       "\n",
       "                    date             Week  Month  \\\n",
       "0    12/31/2019 14:52:18  Month_12 Week 4     12   \n",
       "1    12/30/2019 02:19:02  Month_12 Week 4     12   \n",
       "2    12/26/2019 12:29:53  Month_12 Week 4     12   \n",
       "3    12/23/2019 09:46:31  Month_12 Week 4     12   \n",
       "4    12/20/2019 17:25:01  Month_12 Week 3     12   \n",
       "5    12/20/2019 16:59:59  Month_12 Week 3     12   \n",
       "6    12/17/2019 22:27:18  Month_12 Week 3     12   \n",
       "7    12/17/2019 15:57:25  Month_12 Week 3     12   \n",
       "8    12/17/2019 15:20:24  Month_12 Week 3     12   \n",
       "9    12/15/2019 05:40:59  Month_12 Week 3     12   \n",
       "10   12/14/2019 12:32:19  Month_12 Week 2     12   \n",
       "11   12/13/2019 18:03:47  Month_12 Week 2     12   \n",
       "12   12/13/2019 13:02:25  Month_12 Week 2     12   \n",
       "13   12/12/2019 18:56:29  Month_12 Week 2     12   \n",
       "14   12/12/2019 18:52:08  Month_12 Week 2     12   \n",
       "15   12/11/2019 12:14:55  Month_12 Week 2     12   \n",
       "16   12/10/2019 13:35:16  Month_12 Week 2     12   \n",
       "17   12/10/2019 00:14:44  Month_12 Week 2     12   \n",
       "18   12/06/2019 14:31:19  Month_12 Week 1     12   \n",
       "19   12/05/2019 17:36:18  Month_12 Week 1     12   \n",
       "20   12/04/2019 21:19:31  Month_12 Week 1     12   \n",
       "21   12/04/2019 15:10:02  Month_12 Week 1     12   \n",
       "22   12/04/2019 11:07:22  Month_12 Week 1     12   \n",
       "23   12/02/2019 15:57:43  Month_12 Week 1     12   \n",
       "24   12/02/2019 02:32:02  Month_12 Week 1     12   \n",
       "25   12/01/2019 18:49:06  Month_12 Week 1     12   \n",
       "26   11/30/2019 08:52:00  Month_11 Week 4     11   \n",
       "27   11/29/2019 01:00:06  Month_11 Week 4     11   \n",
       "28   11/28/2019 17:02:49  Month_11 Week 4     11   \n",
       "29   11/28/2019 16:26:57  Month_11 Week 4     11   \n",
       "..                   ...              ...    ...   \n",
       "571  07/15/2019 00:06:23  Month_07 Week 3      7   \n",
       "572  07/14/2019 18:57:38  Month_07 Week 2      7   \n",
       "573  07/14/2019 18:32:00  Month_07 Week 2      7   \n",
       "574  07/14/2019 14:03:09  Month_07 Week 2      7   \n",
       "575  07/13/2019 18:54:01  Month_07 Week 2      7   \n",
       "576  07/12/2019 15:13:32  Month_07 Week 2      7   \n",
       "577  07/12/2019 13:51:35  Month_07 Week 2      7   \n",
       "578  07/12/2019 13:20:20  Month_07 Week 2      7   \n",
       "579  07/12/2019 00:06:58  Month_07 Week 2      7   \n",
       "580  07/11/2019 22:03:02  Month_07 Week 2      7   \n",
       "581  07/11/2019 16:55:02  Month_07 Week 2      7   \n",
       "582  07/11/2019 15:05:18  Month_07 Week 2      7   \n",
       "583  07/11/2019 13:37:46  Month_07 Week 2      7   \n",
       "584  07/11/2019 12:01:12  Month_07 Week 2      7   \n",
       "585  07/11/2019 06:10:01  Month_07 Week 2      7   \n",
       "586  07/11/2019 05:39:33  Month_07 Week 2      7   \n",
       "587  07/10/2019 22:22:05  Month_07 Week 2      7   \n",
       "588  07/10/2019 21:37:25  Month_07 Week 2      7   \n",
       "589  07/10/2019 15:53:17  Month_07 Week 2      7   \n",
       "590  07/10/2019 14:05:17  Month_07 Week 2      7   \n",
       "591  07/10/2019 13:35:56  Month_07 Week 2      7   \n",
       "592  07/10/2019 11:07:39  Month_07 Week 2      7   \n",
       "593  07/10/2019 09:10:36  Month_07 Week 2      7   \n",
       "594  07/10/2019 02:31:28  Month_07 Week 2      7   \n",
       "595  07/09/2019 23:14:11  Month_07 Week 2      7   \n",
       "596  07/09/2019 16:17:38  Month_07 Week 2      7   \n",
       "597  07/09/2019 16:07:31  Month_07 Week 2      7   \n",
       "598  07/09/2019 15:38:01  Month_07 Week 2      7   \n",
       "599  07/09/2019 04:42:55  Month_07 Week 2      7   \n",
       "600  07/09/2019 00:43:20  Month_07 Week 2      7   \n",
       "\n",
       "                                   tweet_clean_textual  \\\n",
       "0    toronto begin waterfront city time lose expres...   \n",
       "1    example toronto specifically quayside portland...   \n",
       "2    sidewalk lab collect manage data rubbish dispo...   \n",
       "3    sidewalk lab list detail plan collect use prop...   \n",
       "4    work mobility look way design street quayside ...   \n",
       "5    colleague say rarely straight line big complex...   \n",
       "6                           waterfront toronto partner   \n",
       "7    company toronto hand entire neighborhood suppo...   \n",
       "8    engineer manager passively accede one suppose ...   \n",
       "9    guy need know ultimate bos sidewalk larry page...   \n",
       "10   make think right slow shouldnt rush sidewalk e...   \n",
       "11   culture fear wan transparency inside vendor fi...   \n",
       "12   toronto google sidewalk lab slowly privatize m...   \n",
       "13   quayside project toronto court controversy exc...   \n",
       "14   privacy alphabet inc determine smart city toro...   \n",
       "15   tech city democracy quayside project toronto r...   \n",
       "16   alphabet sidewalk lab plaza venture establish ...   \n",
       "17   digital infrastructure plan ultimately use ass...   \n",
       "18   toronto quayside model city future develop spe...   \n",
       "19   great talk morning amp future development quay...   \n",
       "20   finally reading age surveillance capitalism ea...   \n",
       "21   dan doctoroff expose flubbed nyc shrivel bosto...   \n",
       "22   hard cost affordable housing toronto best spen...   \n",
       "23   sidewalk lab dangles venture capital fund toro...   \n",
       "24   valuable additional context alphabet google sh...   \n",
       "25   special new today one fav guest back show disc...   \n",
       "26   new york quayside project toronto show cusp ma...   \n",
       "27   sidewalk lab dangles venture capital fund toro...   \n",
       "28   join lab campaign toronto part include support...   \n",
       "29   sidewalk lab dangles venture capital fund toro...   \n",
       "..                                                 ...   \n",
       "571  company produce plan far beyond acre quayside ...   \n",
       "572  caution recognize waterfront toronto receive v...   \n",
       "573  submit proposal call master innovation develop...   \n",
       "574                        toronto tomorrow metafilter   \n",
       "575  think canadian risk right third party able acc...   \n",
       "576  waterfront toronto host series public consulta...   \n",
       "577  huge long last implication smartcities convers...   \n",
       "578  sound like fairly accurate description organic...   \n",
       "579  land prove concept wtf comment left resident v...   \n",
       "580  thank berlin friend show queen show barcelona ...   \n",
       "581  true google toronto well need give away public...   \n",
       "582  summer host series public consultation across ...   \n",
       "583  benefit botnet promote sidewalk lab toronto pr...   \n",
       "584  waterfront toronto host public consultation si...   \n",
       "585  find aila tasmanian award winner read excite q...   \n",
       "586  solidarity amp struggle people lab attempt bui...   \n",
       "587  honor host presentation tonight charlotte matt...   \n",
       "588  san francisco resents techie techie hate fate ...   \n",
       "589  toronto sidewalk lab master plan quayside prov...   \n",
       "590                    release master plan toronto new   \n",
       "591  waterfront toronto input proposal quayside kno...   \n",
       "592  pretty good reading pda say collaboration city...   \n",
       "593  toronto city council warn quayside highly evol...   \n",
       "594                                    gord heard folk   \n",
       "595  tonight waterfront toronto host quayside stake...   \n",
       "596  lol venture capital firm say sidewalk quayside...   \n",
       "597  pleased hear innovation ecosystem toronto stil...   \n",
       "598  discus upside first half show cover propose sm...   \n",
       "599  fpf member news sidewalk lab release detailed ...   \n",
       "600  hey doins transpirin reality alphabet google s...   \n",
       "\n",
       "                                 tweet_clean_sentiment  \n",
       "0    toronto begin waterfront city over time lose e...  \n",
       "1    anyone any more example toronto specifically q...  \n",
       "2    sidewalk lab will collect manage data rubbish ...  \n",
       "3    sidewalk lab list detail plan collect use prop...  \n",
       "4    work mobility look way can design street quays...  \n",
       "5    colleague say rarely straight line big complex...  \n",
       "6                           waterfront toronto partner  \n",
       "7    company toronto want hand entire neighborhood ...  \n",
       "8    engineer manager who passively accede one supp...  \n",
       "9    what guy need know ultimate bos sidewalk larry...  \n",
       "10   make think right slow should not_rush sidewalk...  \n",
       "11   culture fear wan transparency inside vendor fi...  \n",
       "12   toronto google sidewalk lab slowly privatize m...  \n",
       "13   quayside project toronto court more controvers...  \n",
       "14   no_privacy alphabet inc determine smart city t...  \n",
       "15   tech city democracy quayside project toronto r...  \n",
       "16   alphabet sidewalk lab plaza venture establish ...  \n",
       "17   digital infrastructure plan will ultimately us...  \n",
       "18   toronto quayside model city future develop spe...  \n",
       "19   great talk morning with amp future development...  \n",
       "20   finally reading age surveillance capitalism ea...  \n",
       "21   dan doctoroff get expose again flubbed nyc shr...  \n",
       "22   hard cost affordable housing toronto best spen...  \n",
       "23   sidewalk lab dangles venture capital fund toro...  \n",
       "24   valuable additional context alphabet google sh...  \n",
       "25   very special new today one fav guest back show...  \n",
       "26   new york quayside project toronto show cusp ma...  \n",
       "27   sidewalk lab dangles venture capital fund toro...  \n",
       "28   join lab campaign toronto part include support...  \n",
       "29   sidewalk lab dangles venture capital fund toro...  \n",
       "..                                                 ...  \n",
       "571  company produce plan far beyond acre quayside ...  \n",
       "572  caution high voltage high voltage high voltage...  \n",
       "573  submit proposal call master innovation develop...  \n",
       "574                        toronto tomorrow metafilter  \n",
       "575  think canadian risk right third party would ab...  \n",
       "576  waterfront toronto host series public consulta...  \n",
       "577  huge long last implication smartcities convers...  \n",
       "578  sound like fairly accurate description organic...  \n",
       "579  more land prove concept wtf comment left resid...  \n",
       "580  thank berlin friend show can queen show can ba...  \n",
       "581  will true google toronto well not_need give aw...  \n",
       "582  summer will host series public consultation ac...  \n",
       "583  who benefit botnet promote sidewalk lab toront...  \n",
       "584  waterfront toronto host public consultation si...  \n",
       "585  find out aila tasmanian award winner read exci...  \n",
       "586  solidarity amp struggle people against lab att...  \n",
       "587  honor host presentation tonight charlotte matt...  \n",
       "588  san francisco resents techie techie hate fate ...  \n",
       "589  toronto sidewalk lab master plan quayside prov...  \n",
       "590                    release master plan toronto new  \n",
       "591  waterfront toronto want input proposal quaysid...  \n",
       "592  pretty good reading pda where say collaboratio...  \n",
       "593  toronto city council warn quayside most highly...  \n",
       "594                                    gord heard folk  \n",
       "595  tonight waterfront toronto host quayside stake...  \n",
       "596  lol venture capital firm say sidewalk quayside...  \n",
       "597  pleased hear innovation ecosystem toronto stil...  \n",
       "598  discus upside first half show cover propose sm...  \n",
       "599  fpf member news sidewalk lab release detailed ...  \n",
       "600  hey doin transpirin reality alphabet google sw...  \n",
       "\n",
       "[601 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_sentiment(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
